{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the Hugging Face token from the .env file\n",
    "load_dotenv()\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of pdf pages  1299\n",
      "10977\n",
      "WARNING:tensorflow:From c:\\Users\\Arsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "# Step1 : Load Raw Pdf \n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "# For Raw pdf loading \n",
    "\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "def load_pdf_files(data):\n",
    "    loader = DirectoryLoader(data,\n",
    "                             glob=\"*.pdf\",\n",
    "                             loader_cls=PyPDFLoader)\n",
    "    \n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "documents = load_pdf_files(data=DATA_PATH)\n",
    "print(\"Length of pdf pages \" , len(documents))\n",
    "\n",
    "# All the pages are loaded .\n",
    "\n",
    "# Step2 : Create Chunks \n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# For creating Chunks\n",
    "def create_chunks(extracted_data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=50)\n",
    "    text_chunks = text_splitter.split_documents(extracted_data)\n",
    "    return text_chunks\n",
    "\n",
    "text_chunks = create_chunks(extracted_data=documents)\n",
    "print(len(text_chunks))\n",
    "\n",
    "\n",
    "# Step3 : Create vector Embeddings\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def get_embedding_model():\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embedding_model\n",
    "# used for clustering or semantic search \n",
    "\n",
    "embedding_model = get_embedding_model()\n",
    "\n",
    "# Step4 : Store Embeddings in FAISS\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "DB_FAISS_PATH = \"vectorstore/faiss_db\"\n",
    "db=FAISS.from_documents(text_chunks, embedding_model)\n",
    "# in chunks ko is embedding model ke through embedding bana do \n",
    "db.save_local(DB_FAISS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the pages / Documents > Chunk > Vector Embed > Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect Memory with LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Set Up LLM\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "HUGGINGFACE_REPO_ID=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "def load_llm(huggingface_repo_id):\n",
    "    llm = HuggingFaceEndpoint(repo_id = huggingface_repo_id,temperature=0.5,model_kwargs={\"token\":HF_TOKEN,\"max_length\": 512})\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "# Step 2 : Connect LLM with FAISS and create Chain\n",
    "\n",
    "CUSTOM_PROMPT_TEMPLATE = \"\"\"\n",
    "                Use the pieces of information provided in the context to answer user's question.\n",
    "                If you dont know the answer, just say that you dont know, dont try to make up an answer. \n",
    "                Dont provide anything out of the given context\n",
    "\n",
    "                Context: {context}\n",
    "                Question: {question}\n",
    "\n",
    "                Start the answer directly. No small talk please.\n",
    "                \"\"\"\n",
    "\n",
    "def set_custom_prompt(custom_prompt_template):\n",
    "    prompt=PromptTemplate(template=custom_prompt_template, input_variables=[\"context\", \"question\"])\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Load Database \n",
    "\n",
    "DB_FAISS_PATH = \"vectorstore/faiss_db\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = FAISS.load_local(DB_FAISS_PATH,embedding_model,allow_dangerous_deserialization=True)\n",
    "\n",
    "# Create QA Chain'\n",
    "\n",
    "qa_chain=RetrievalQA.from_chain_type(llm=load_llm(HUGGINGFACE_REPO_ID),\n",
    "                                      chain_type='stuff',\n",
    "                                       retriever= db.as_retriever(search_kwargs={'k ': 3 }),\n",
    "                                       return_source_documents=True ,\n",
    "                                       chain_type_kwargs = {'prompt':set_custom_prompt( CUSTOM_PROMPT_TEMPLATE)})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Arsh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT :     Cancer can be cured through various treatments such as surgery, chemotherapy, radiation therapy, and in some cases, the use of cancer vaccines. The best chance for a surgical cure is usually with the first operation. However, it's important to note that the success of these treatments can vary greatly depending on the type and stage of the cancer, as well as the individual's overall health. It's always best to consult with a healthcare professional for personalized advice.\n",
      "source documents :  [Document(id='01d129eb-5274-453c-95d3-e511ffb3f584', metadata={'source': 'data\\\\The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 26}, page_content='curative for some stomach, genital/urinary, thyroid,\\nbreast, skin, and central nervous system cancers. The best\\nchance for a surgical cure is usually with the first opera-\\nGALE ENCYCLOPEDIA OF MEDICINE 2638\\nCancer therapy, definitive'), Document(id='1017822f-ed18-4c72-a043-28c7a41b5bf3', metadata={'source': 'data\\\\The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 186}, page_content='Treatment\\nChoriocarcinomas are usually treated by surgical\\nremoval of the tumor and chemotherapy . Radiation is\\noccasionally used, particularly for tumors in the brain.\\nAlternative treatment\\nComplementary treatments can decrease stress,\\nreduce the side effects of cancer treatment, and help\\npatients feel more in control. For instance, some people\\nfind activities such as yoga, massage, music therapy ,\\nmeditation, prayer, or mild physical exercise helpful.\\nPrognosis'), Document(id='79706cc8-2499-47d8-b463-d15bc55f1a74', metadata={'source': 'data\\\\anatomy+phys+vol2a.pdf', 'page': 507}, page_content='the disease process.\\nThis fact has led to extensive research in trying to develop ways to enhance the early immune response to completely\\neliminate the early cancer and thus prevent a later escape. One method that has shown some success is the use of cancer\\nvaccines, which differ from viral and bacterial vaccines in that they are directed against the cells of one’s own body.\\nTreated cancer cells are injected into cancer patients to enhance their anti-cancer immune response and thereby prolong'), Document(id='63312dec-ec2a-4a4d-9f39-1ab53094225b', metadata={'source': 'data\\\\The_GALE_ENCYCLOPEDIA_of_MEDICINE_SECOND.pdf', 'page': 131}, page_content='py options may be explored.\\n• A secondary malignancy may develop from the one\\nbeing treated, and that second cancer may need addi-\\ntional chemotherapy or other treatment.\\nResources\\nBOOKS\\nDollinger, Malin, et al. Everyone’s Guide to Cancer Therapy:\\nHow Cancer is Diagnosed, Treated, and Managed Day to\\nDay.3rd ed. Kansas City: Andres & McMeel, 1998.\\nDrum, David. Making the Chemotherapy Decision. Los Ange-\\nles: Lowell House, 1996.\\nKEY TERMS\\nAdjuvant therapy —Treatment given after surgery')]\n"
     ]
    }
   ],
   "source": [
    "# Now invoke the chain with single query \n",
    "\n",
    "user_query = input(\"Enter your query : \")\n",
    "response = qa_chain.invoke({'query':user_query})\n",
    "\n",
    "print('RESULT :' , response['result'])\n",
    "print(\"source documents : \", response['source_documents'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
